# -*- coding: utf-8 -*-
"""Trends Content-based NFT Recommender System.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1gObt7EB6t_qdD4b3Zk9AkYZ2kMqOeaFo

# Trends Content-based NFT Recommender System

https://towardsdatascience.com/how-to-build-from-scratch-a-content-based-movie-recommender-with-natural-language-processing-25ad400eb243

Check this later (not that different): https://medium.com/dataseries/recommendation-system-in-python-4d4cf6d33166 , https://gist.github.com/deansublett/06f010a886831f9dac5b1f9bce4f4229?source=post_page---------------------------
"""

import pandas as pd
from rake_nltk import Rake
import nltk
from datetime import datetime, date


def load_preprocess_data():
  nltk.download('stopwords')
  from nltk.corpus import stopwords
  nltk.download('punkt')

  """## Get Dataset

  ### NFT Data
  """
  assets_file_path = "../../datasets/collected-nft-assets-unique"
  df = pd.read_csv(assets_file_path, sep='\t')

  """## Data Cleaning"""

  for index, row in df.iterrows():
      df.at[index,'reference_id'] = row["asset_contract_address"] + "-" + str(row["nft_id"])

  """Clean duplicate data in dataframe
  """
  # dropping ALL duplicate values
  # df.drop_duplicates(subset ="reference_id", keep = False, inplace = True)

  # dropping ALL duplicate values
  df.drop_duplicates(subset="reference_id", keep='first', inplace = True)

  """## Rake Vectorizer
  Vectorize words
  """

  def vectorize_and_extract_key_words_in_column( column_name ):

    # initializing the new column
    new_column_name = 'Key_words_' + column_name
    df[new_column_name] = ""

    for index, row in df.iterrows():
        column_cell = row[column_name]

        
        # check if the cell has a string, to apply NLP
        if isinstance(column_cell, str):
          # instantiating Rake, by default it uses english stopwords from NLTK
          # and discards all puntuation characters as well
          r = Rake()

          # extracting the words by passing the text
          r.extract_keywords_from_text(column_cell)

          # getting the dictionary with key words as keys and their scores as values
          key_words_dict_scores = r.get_word_degrees()

          # print(key_words_dict_scores.keys())

          # assigning the key words to the new column for the corresponding NFT
          # row[new_column_name] = list(key_words_dict_scores.keys()) # this doesn't work for some reason

          df.at[index, new_column_name]=list(key_words_dict_scores.keys())

        else:
          df.at[index, new_column_name] = None

    # dropping the column_name column
    df.drop(columns = [column_name], inplace = True)

  vectorize_and_extract_key_words_in_column('name')
  vectorize_and_extract_key_words_in_column('asset_description')
  vectorize_and_extract_key_words_in_column('collection_name')
  vectorize_and_extract_key_words_in_column('collection_description')


  """### Combine all key words into one column

  This may come handy when comparing with trends data.
  """

  columns = df.columns
  df['All_key_words_list'] = pd.NA
  df['All_key_words_str'] = ''

  for index, row in df.iterrows():
    words = []
    key_words_str=''

    for col in columns:
      if col.startswith("Key_words") and row[col] != None:
        words += row[col]
        key_words_str += ' '.join(row[col])+ ' '

    # row['All_key_words_list'] = words
    # row['All_key_words_str'] = key_words_str
    df.at[index, 'All_key_words_list'] = words
    df.at[index, 'All_key_words_str'] = key_words_str


  """Create `bag_of_words`

  Drop useless columns
  """

  df['All_key_words_str'].count()

  df.set_index('reference_id', inplace = True)   # set reference_id as the index of the dataframe

  df.drop(columns = [col for col in df.columns if not col.startswith("All_key_words")], inplace = True)


  """## Modeling"""

  """### Trends based Content maching"""

  # trending_phrases = ['ROMCOM SERIES FOR HAESOO', 'Golden Corral', 'Wordle 228', 'Tietê', 'BOP TRACKLIST', '伊東純也', 'JIKJIN FOCUS FILM', 'NINJAPAN 2021', '一気読み', 'ムササビ', 'オーストラリア', 'WONPIL Concept Photo 3', 'Benzine 39', 'あーりん', 'البليهي', 'Kraft', '花枝ちゃん', 'HAPPY BIRTHDAY GEONU', 'ちゃん誕生日', 'サウジアラビア', '도영듀스', '凛ちゃん', 'SECRETCRUSH TRAILER', 'Best Entertainer Jungkook', 'UM CORPO DE VERÃO', 'Quito', 'Damn Brady', 'Schefter', '日村さん', 'SMILE FOR MEW', 'HBCUs', 'もやとらっしー', 'Lucas', '拷問部屋', 'Wordle 244 X', '視聴覚室', '子供部屋', 'Gustavo', '警視庁初摘発', '日本人の覚醒剤製造', '暴力団幹部とYouTuber組員ら逮捕', 'Donda 2', 'Tiago', 'ベランダ', 'Peaky Peaky', '당신 이야기', '대신 전해', '¥930,000', 'RED VELVET MARCH COMEBACK', 'Embiid', 'Jack Harlow', 'sabrina', 'Nonku', 'Giannis', '国民負担率', '¥115,000', '鬱アニメ', "GUESS WHO'S BACK", 'TBS NEWS', '¥139,000', 'sungas', '超大作怪獣映画', 'Middleton', '男性医師の有罪判決', 'Barão da Piscadinha', '自爆デッキ', 'Stem Player', 'はしもっちゃん', 'Aグループ', 'Eslo', 'あやかし譚', 'おててまもる君', 'ミラキュラス', 'Yeat']
  # phrases contains all trending words

  import pprint   # well-formatted print output

  pp = pprint.PrettyPrinter(indent=1)

  """**TODO EXTRA (not that important): Fetch each group of trends at a time from MongoDB/ get min_tweet_volume of only trends fetched at the same `created_datetime`**"""

  # Connect to MongoDB
  import pymongo

  mongo_client = pymongo.MongoClient("mongodb+srv://admin:2KRv1funPDgawFu7@recsyscluster.lvxb6.mongodb.net")
  mydb = mongo_client["nft-recsys"]
  trends_collection = mydb["trends"]

  twitter_trends = []

  for x in trends_collection.find():
    # pp.pprint(x)
    twitter_trends.append(x)

  len(twitter_trends)

  """*rethink if there's a benefit before attempting:* Preprocess batch-wise (based on created_timestamp of the trend)

  to avoid majority of the data getting the same score

      #  TODO (Good to have): split words that are combined - usually happens for hashtags

  """

  pre_processed_twitter_trends = []

  bag_of_trends_phrases = []
  min_tweet_volume = None

  for trend in twitter_trends:
    if trend['name'] not in bag_of_trends_phrases :
      # ignore duplicates (twitter API bug? sometimes trends are duplicated) 
      bag_of_trends_phrases.append(trend['name'])

      pre_processed_trend = trend

      #  remove hashtags
      if trend['name'][0:1] == '#':
        # remove hashtag
        pre_processed_trend['name'] = trend['name'][1:]

      # update min_tweet_volume
      if trend['tweet_volume']:
        if min_tweet_volume == None:
          # first trend which has a tweet_volume
          min_tweet_volume = trend['tweet_volume']
        elif trend['tweet_volume'] < min_tweet_volume:
          # update min_tweet_volume
          min_tweet_volume = trend['tweet_volume']

      #  convert name to lower case
      pre_processed_trend['name'] = pre_processed_trend['name'].lower()

      pre_processed_twitter_trends.append(pre_processed_trend)

  # add min tweet volume for tweets with no volume
  for index, trend in enumerate(pre_processed_twitter_trends):
    if trend['tweet_volume'] == None:
        pre_processed_twitter_trends[index]['tweet_volume'] = min_tweet_volume - 1


  # print(pre_processed_twitter_trends)
  # pp.pprint(pre_processed_twitter_trends)

  """Calculate Median Tweet volume

  An impact score can be calculated separately as well.
  """

  import statistics

  # add all tweets with tweet volumes into an array
  tweet_volumes_array = []
  for tweet in pre_processed_twitter_trends:
    if tweet['tweet_volume'] != None:
      tweet_volumes_array.append(tweet['tweet_volume'])

  print("tweet_volumes_array:", tweet_volumes_array)

  # calculate median tweet volume
  median_tweet_volume = statistics.median(tweet_volumes_array)
  print(median_tweet_volume)

  """*Date difference calculation*

  ref: https://stackoverflow.com/a/151211/11005638
  """

  # trend_datetime = twitter_trends[-1]['created_datetime']
  # print(trend_datetime)

  # d0 = date(2021, 8, 18)
  # d0 = trend_datetime.date()
  # d1 = datetime.now().date()

  def get_date_diff(d0, d1):
    delta = d1 - d0
    # print(delta.days)
    return delta.days

  # get_date_diff(d0, datetime.now().date())
  # get_date_diff(trend_datetime.date(), datetime.now().date())

  """calculate `trend_score`"""

  def calculate_trend_score(date_diff, trend_volume, median_tweet_volume, sentiment):
    mu = 0.1 # constant

    trend_impact_score = (trend_volume/ median_tweet_volume)

    trend_score = sentiment * trend_impact_score/ (mu + date_diff)
    return trend_score

  """Calculate `trend_score` for each trend"""

  current_date = datetime.now().date()

  def calculate_trend_score_for_all_trends(curr_date = current_date):
    for trend in pre_processed_twitter_trends:
      volume = trend['tweet_volume']
      # trend_impact_score = (volume/ median_tweet_volume)

      trend_datetime = trend['created_datetime']
      date_diff = get_date_diff(trend_datetime.date(), curr_date)

      sentiment = 1 # will be taken 1 (which is in between neutral and positive) if sentiment hasn't been calculated

      if 'sentiment_score' in trend:
        sentiment = trend['sentiment_score']
      
      trend_score = calculate_trend_score(date_diff, volume, median_tweet_volume, sentiment)
      trend['trend_score'] = trend_score

  calculate_trend_score_for_all_trends()

  """Give a score for each row (NFT) based on the matching content"""

  df['trend_score'] = pd.NA
  df['matched_trends'] = pd.NA
  # not_interested_trends = []  # defined by admin/ each user?

  for index, row in df.iterrows():
    new_trend_match_score = 0
    matched_trends_to_item = []

    for trend in pre_processed_twitter_trends:

      if trend['name'] in row['All_key_words_list']:
      # if the content matches

        # calculate trend_score for each trend before trying to match because this score will be the same for all content - total score will be an aggregate
        new_trend_match_score += trend['trend_score']

        # add trend to matched_trends_to_item
        matched_trends_to_item.append(trend['name'])

        print(trend, "\n",row)

    df.at[index, 'trend_score'] = new_trend_match_score
    # save list of trends matched with item
    df.at[index, 'matched_trends'] = matched_trends_to_item




  """display items that have trends that were matched"""

  # get items that only have a trend_score (greater than zero, to not get negative ranked sentiment items at all)
    # This condition depends on the implementation (to to find a score/ method to rank these values in the middle with zeros (item-to-item matches? based on categorical content in trends/ trending items?), 
    # because whatever trend it is, even negative sentiment might be better than no sentiment - this affects precision or recall)
    # trending_df = df[df["trend_score"] > 0 ]
  global trending_df
  trending_df = df[df['trend_score'] != 0]


load_preprocess_data()


"""### Get Trends based Recommendations sorted by `trend_score`"""

def trends_based_recommendations():
  # sort dataframe based on trend_score
  top_trending_df = trending_df.sort_values(by=['trend_score'], ascending=False)

  top_trending_items = list(pd.Series(top_trending_df.index))
  
  #TODO: this may have to return all the data: would be better to in all recsys models - have a param to check if required?
  
  return top_trending_items



"""## Testing"""
top_trending_items = trends_based_recommendations()
print("top_trending_items", top_trending_items)


# get other testing/ eval functions from colab notebook if required



# TODO: top featured items per day: try to give a mark/ separate today's (last matched batch) trending matches or top 4/5