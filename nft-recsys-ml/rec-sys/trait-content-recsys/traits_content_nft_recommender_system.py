# -*- coding: utf-8 -*-
"""Traits Content & Rarity based NFT Recommender System.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1iBz6SKCscjxowrEKL-j9teMF24Ul6x3L

# Traits Content-based NFT Recommender System

https://towardsdatascience.com/how-to-build-from-scratch-a-content-based-movie-recommender-with-natural-language-processing-25ad400eb243

Check this later (not that different): https://medium.com/dataseries/recommendation-system-in-python-4d4cf6d33166 , https://gist.github.com/deansublett/06f010a886831f9dac5b1f9bce4f4229?source=post_page---------------------------
"""
"""
TODO:
have data pre-processing and loading on startup of the service, regenerate based on request from frontend
separate rarity & content into two separate services (folders here)
"""


# TODO: install pandas, numpy, sklearn
import pandas as pd
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.feature_extraction.text import CountVectorizer


"""## Get Dataset"""

# TODO: remove colab imports, change to local csvs (do this for now) or mongo(later, if time permites)?
from google.colab import drive
drive.mount('drive')

assets_file_path = "/content/drive/MyDrive/Datasets/FYP/bayc-nft-assets"
df = pd.read_csv(assets_file_path, sep='\t')

# df = df[['nft_id','traits_string','asset_contract_address', 'total_rarity']]
# df.head()

"""## Data Cleaning"""

for index, row in df.iterrows():
    df.at[index,'reference_id'] = row["asset_contract_address"] + "-" + str(row["nft_id"])

df['traits_string'] = df['traits_string'].str.replace(';',' ')

# df.head()

# df.iloc[0]


df.set_index('reference_id', inplace = True)   # set reference_id as the index of the dataframe
# df.head()

# df.drop(columns = [col for col in df.columns if ((col!= 'traits_string') or (col!= 'total_rarity'))], inplace = True)
df = df[['traits_string','total_rarity']]
# df.head()

# df['traits_string'].count()
# df['total_rarity'].count()

"""## Modeling

In order to detect similarities between nfts, I need to vectorize, as I mentioned above. I decided to use CountVectorizer rather than TfIdfVectorizer for one simple reason: I need a simple frequency counter for each word in my bag_of_words column. Tf-Idf tends to give less importance to the words that are more present in the entire corpus (our whole column, in this case) which is not what we want for this application, because every word is important to detect similarity! 

*It seems like Tf-Idf would give more meaningful results* for ranking - check this later. For relevance, CountVectorizer seems to be ok.

Once I have the matrix containing the count for each word, we can apply the cosine_similarity function
"""

# instantiating and generating the count matrix
count = CountVectorizer()   # used to transform a given text into a vector on the basis of the frequency (count) of each word that occurs in the entire text
count_matrix = count.fit_transform(df['traits_string'])

# print(count_matrix)
count_matrix.todense()

count_matrix.shape

"""in the count_matrix, i seems to be the NFT, j seems to be each word in the bag_of_words per NFT. the value at i, j gives the frequency of each word in the entire column of bag_of_words.

For some reason (need to find out) this frequency is needed to calculate the cosine similarity between the content in NFTs. - *The definition of similarity between two vectors u and v is, in fact, the ratio between their dot product and the product of their magnitudes.*
"""

# creating a Series for the reference_id so they are associated to an ordered numerical
# list I will use later to match the indexes - easy to refer the NFT by index from the array that is created (cosine_sim)
indices = pd.Series(df.index)
indices[:5]

"""Check if this concern can be addressed: Having cosine_sim in-memory can be a memory constraint for larger datasets"""

# generating the cosine similarity matrix
cosine_sim = cosine_similarity(count_matrix, count_matrix)
# cosine_sim.todense()

# function that takes in reference_id as input and returns the top 10 recommended nfts
def content_based_recommendations(reference_id, cosine_sim = cosine_sim):
    
    recommended_nfts = []
    cosine_sim_scores_of_recommendations = []

    # getting the index of the NFT that matches the reference_id
    idx = indices[indices == reference_id].index[0]

    # creating a Series with the similarity scores in descending order
    score_series = pd.Series(cosine_sim[idx]).sort_values(ascending = False)

    # getting the indexes of the 10 most similar nfts
    top_10_indexes = list(score_series.iloc[1:11].index)
    # getting the cosine similarities of the 10 most similar nfts
    cosine_sim_scores_of_recommendations = list(score_series.iloc[1:11])
    
    # populating the list with the reference_ids of the best 10 matching nfts
    for i in top_10_indexes:
        recommended_nfts.append(list(df.index)[i])
        
    return recommended_nfts, cosine_sim_scores_of_recommendations

def trait_rarity_recommendations(reference_id):

    recommended_nfts = []
    trait_rarity_scores_of_recommendations = []

    input = df.loc[reference_id]['total_rarity']
    # print(input)

    #  This considers the entire dataframe. Need to do this only within a collection - send the filtered dataframe as a parameter
    # the dataframe with 10 closest values.
    df_sort = df.iloc[(df['total_rarity']-input).abs().argsort()[1:11]]

    recommended_nfts = df_sort.index.tolist()
    trait_rarity_scores_of_recommendations = df_sort['total_rarity'].tolist()
    # print(df_sort['total_rarity'].tolist())

    return recommended_nfts, trait_rarity_scores_of_recommendations




"""## Testing

Recommendation based on content similarity of traits matches
"""

content_based_recommendations('0xbc4ca0eda7647a8ab7c2061c2e118a18a936f13d-9948')

"""Recommendation based on total rarity score similarity"""

trait_rarity_recommendations('0xbc4ca0eda7647a8ab7c2061c2e118a18a936f13d-9948')



#  TODO: (Extra) have these as separate API endpoints - to get later, if required
# def get_cosine_sim(initial_reference, item_reference_id):
#     # getting the index of the initial NFT that was used for recommendations
#     initial_idx = indices[indices == initial_reference].index[0]
#     recommended_idx = indices[indices == item_reference_id].index[0]

#     return cosine_sim[initial_idx][recommended_idx]

# def get_total_rarity_score(item_reference_id):
#     return df.loc[item_reference_id]['total_rarity']

# # define this at the top, before getting recommendations

# main_reference_item = "0xbc4ca0eda7647a8ab7c2061c2e118a18a936f13d-9948"

