# -*- coding: utf-8 -*-
"""Basic Content-based NFT Recommender System.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Fte4_0X6xvYhxa0tZcQ4QeiklAfMnIFB

# Basic Content-based NFT Recommender System

https://towardsdatascience.com/how-to-build-from-scratch-a-content-based-movie-recommender-with-natural-language-processing-25ad400eb243

Check this later (not that different): https://medium.com/dataseries/recommendation-system-in-python-4d4cf6d33166 , https://gist.github.com/deansublett/06f010a886831f9dac5b1f9bce4f4229?source=post_page---------------------------
"""

import pandas as pd
from rake_nltk import Rake
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.feature_extraction.text import CountVectorizer
import nltk


def load_preprocess_data():
  nltk.download('stopwords')
  from nltk.corpus import stopwords
  nltk.download('punkt')

  """## Get Dataset"""

  assets_file_path = "../../datasets/collected-nft-assets"
  global df
  df = pd.read_csv(assets_file_path, sep='\t')

  # df = df[['nft_id','traits_string','asset_contract_address', 'total_rarity']]
  df.head()

  """## Data Cleaning"""

  for index, row in df.iterrows():
      df.at[index,'reference_id'] = row["asset_contract_address"] + "-" + str(row["nft_id"])

  """## Rake (Rapid Automatic Keyword Extraction)
  Extract keywords
  """

  def extract_keywords_in_column( column_name ):

    # initializing the new column
    new_column_name = 'Key_words_' + column_name
    df[new_column_name] = ""

    for index, row in df.iterrows():
        column_cell = row[column_name]

        
        # check if the cell has a string, to apply NLP
        if isinstance(column_cell, str):
          # instantiating Rake, by default it uses english stopwords from NLTK
          # and discards all puntuation characters as well
          r = Rake()

          # extracting the words by passing the text
          r.extract_keywords_from_text(column_cell)

          # getting the dictionary with key words as keys and their scores as values
          key_words_dict_scores = r.get_word_degrees()

          # print(key_words_dict_scores.keys())

          # assigning the key words to the new column for the corresponding movie
          # row[new_column_name] = list(key_words_dict_scores.keys()) # this doesn't work for some reason

          df.at[index, new_column_name]=list(key_words_dict_scores.keys())

        else:
          df.at[index, new_column_name] = None

    # dropping the column_name column
    df.drop(columns = [column_name], inplace = True)

  extract_keywords_in_column('name')
  extract_keywords_in_column('asset_description')
  extract_keywords_in_column('collection_name')
  extract_keywords_in_column('collection_description')


  """### Combine all key words into one column

  This may come handy when comparing with trends data.
  """

  columns = df.columns
  df['All_key_words_list'] = pd.NA
  df['All_key_words_str'] = ''

  for index, row in df.iterrows():
    words = []
    key_words_str=''

    for col in columns:
      if col.startswith("Key_words") and row[col] != None:
        words += row[col]
        key_words_str += ' '.join(row[col])+ ' '

    # row['All_key_words_list'] = words
    # row['All_key_words_str'] = key_words_str
    df.at[index, 'All_key_words_list'] = words
    df.at[index, 'All_key_words_str'] = key_words_str

  """Create `bag_of_words`

  Drop useless columns
  """

  df['All_key_words_str'].count()

  df.set_index('reference_id', inplace = True)   # set reference_id as the index of the dataframe

  df.drop(columns = [col for col in df.columns if not col.startswith("All_key_words")], inplace = True)

  """## Modeling

  In order to detect similarities between NFTs, I need to vectorize, as I mentioned above. I decided to use CountVectorizer rather than TfIdfVectorizer for one simple reason: I need a simple frequency counter for each word in my bag_of_words column. Tf-Idf tends to give less importance to the words that are more present in the entire corpus (our whole column, in this case) which is not what we want for this application, because every word is important to detect similarity! 

  *It seems like Tf-Idf would give more meaningful results* for ranking - check this later. For relevance, CountVectorizer seems to be ok.

  Once I have the matrix containing the count for each word, we can apply the cosine_similarity function
  """

  # instantiating and generating the count matrix
  count = CountVectorizer()   # used to transform a given text into a vector on the basis of the frequency (count) of each word that occurs in the entire text
  count_matrix = count.fit_transform(df['All_key_words_str'])

  # print(count_matrix)
  count_matrix.todense()

  count_matrix.shape

  """in the count_matrix, i seems to be the movie, j seems to be each word in the bag_of_words per movie. the value at i, j gives the frequency of each word in the entire column of bag_of_words.

  For some reason (need to find out) this frequency is needed to calculate the cosine similarity between the content in movies. - *The definition of similarity between two vectors u and v is, in fact, the ratio between their dot product and the product of their magnitudes.*
  """

  # creating a Series for the movie titles so they are associated to an ordered numerical
  # list I will use later to match the indexes - easy to refer the movie title by index from the array that is created (cosine_sim)
  global indices
  indices = pd.Series(df.index)
  indices[:5]

  """Check if this concern can be addressed: Having cosine_sim in-memory can be a memory constraint for larger datasets"""

  # generating the cosine similarity matrix
  global cosine_sim
  cosine_sim = cosine_similarity(count_matrix, count_matrix)
  # cosine_sim.todense()

load_preprocess_data()  # FIXME: this can't be removed, running the code in __main__ won't work well enough
# having this here solves this for now, it somehow works as expected.
# everything is loaded on startup


# function that takes in reference_id (nft_contract_address-nft_id) as input and returns the top 10 recommended nfts
def description_content_based_recommendations(reference_id, cosine_sim = cosine_sim):
    
    recommended_nfts = []
    cosine_sim_scores_of_recommendations = []

    # getting the index of the nft that matches the reference_id
    idx = indices[indices == reference_id].index[0]

    # creating a Series with the similarity scores in descending order
    score_series = pd.Series(cosine_sim[idx]).sort_values(ascending = False)

    # getting the indexes of the 10 most similar nfts
    top_10_indexes = list(score_series.iloc[1:11].index)
    # getting the cosine similarities of the 10 most similar nfts
    cosine_sim_scores_of_recommendations = list(score_series.iloc[1:11])

    # populating the list with the reference_ids of the best 10 matching nfts
    for i in top_10_indexes:
        recommended_nfts.append(list(df.index)[i])
        
    return recommended_nfts, cosine_sim_scores_of_recommendations


"""## Testing"""

# resp = description_content_based_recommendations('0x495f947276749ce646f68ac8c248420045cb7b5e-57422959511997337577873730268633988669226471548944456734432444570123223695361')
# print("description_content_based_recommendations", resp)